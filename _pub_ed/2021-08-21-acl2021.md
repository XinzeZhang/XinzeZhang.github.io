---
layout: article
title: Crafting Adversarial Examples for Neural Machine Translation. In Proceedings of ACL, 2021.
permalink: /published/acl2021.html
key: ed-acl2021
# show_info: true
# modify_date: 2021-08-05
---
Xinze Zhang, Junzhe Zhang, Zhenhua Chen, and Kun He. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL). 2021.

<!--more-->

### Abstract

<div style="text-align: justify"   markdown='1'>
Effective adversary generation for neural machine translation (NMT) is a crucial prerequisite for building robust machine translation systems. In this work, we investigate veritable evaluations of NMT adversarial attacks, and propose a novel method to craft NMT adversarial examples. We first show the current NMT adversarial attacks may be improperly estimated by the commonly used mono-directional translation, and we propose to leverage the round-trip translation technique to build valid metrics for evaluating NMT adversarial attacks. Our intuition is that an effective NMT adversarial example, which imposes minor shifting on the source and degrades the translation dramatically, would naturally lead to a semantic-destroyed round-trip translation result. We then propose a promising black-box attack method called Word Saliency speedup Local Search (WSLS) that could effectively attack the mainstream NMT architectures. Comprehensive experiments demonstrate that the proposed metrics could accurately evaluate the attack effectiveness, and the proposed WSLS could significantly break the state-of-art NMT models with small perturbation. Besides, WSLS exhibits strong transferability on attacking Baidu and Bing online translators.
</div>
